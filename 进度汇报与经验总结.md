# 进度记录和经验总结
## 本文档用于记录项目搭建的过程，遇到的问题及解决方案，以便后续查阅

## 目录
- [重点文献总结](#index1)
- [配置自动标注软件X-AnyLabeling](#index2)
- [4月26日Q&A记录](#index3)
- [MOT(移动目标跟踪)领域文章阅读概述](#index4)
- [确立Baseline](#index5)

## 重点文献总结 <span id='index1'>
1.`Sundararaman, Ramana, et al. "Tracking pedestrian heads in dense crowd." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.`

论文地址：<https://arxiv.org/pdf/2103.13516>

Github项目地址：<https://github.com/Sentient07/HeadHunter--T>

&emsp; 一篇比较综合的开源文章。提出了一个数据集`CroHD`，一个用于度量标准`IDEucl`，一个头部检测器`HeadHunter`以及以它为基础的头部跟踪方法`HeadHunter-T`。目前已有较多模型使用该数据集，并在结果上超过了本文，不过其中可以找到的开源方法并不多。

&emsp; 注：基于PET的点标注方法只可以用于检测，而跟踪显然需要更多信息。计数任务则介于两者之间，要求时空一致的检测，又不必完全准确地跟踪每个人。本文提供了一个新的思路，即，如果以头部框标注，则有可能结合头部特征进行跟踪，从而降级到人群计数，这和之前的每一帧头像检测，升级到时空一致计数是两个不同的方向。

2.`Cao, Jinkun et al. “Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking.” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 9686-9696.`

论文地址：<https://arxiv.org/pdf/2203.14360>

Github项目地址：<https://github.com/noahcao/OC_SORT>

&emsp; 文章提出的OC-SORT模型是对SORT（Simple online and realtime tracking）的一种改进，增强了其在遮挡和非线性运动(如舞蹈动作跟踪)任务上的表现。为了准确预测，加之大部分MOT数据集要求人体识别，所以其采用了全身的标注框。但是它也在HeadTracking数据集上做了一些工作。

&emsp; 该作者也提出，人群的跟踪基于头部会更加有效。据作者说，该方法在人体特定部位的识别上比之前的方法要更好，尽管使用的OC-SORT在其它MOT数据集上的参数。这一部分提及内容较少，但也许OC-SORT可以作为方法指引。



## 配置自动标注软件X-AnyLabeling <span id='index2'>
&emsp; X-Anylabeling提供手动标注图片视频，及使用AI模型自动标注功能。下面是配置该软件的一些总结。
- 项目地址：<https://github.com/CVHub520/X-AnyLabeling>
- 按照项目README-get_started部分内容安装相关包
- 运行app.py以进入软件的GUI，即可在CPU模式下下载模型并进行标注。标注会在当前目录生成同名json文件
- 配置GPU运行步骤
    1. 将`anylabeling/app_info.py`中的`__preferred_device__` 参数改为GPU
    2. 安装相关包`pip install -r requirements-gpu-dev.txt`
    3. 打开`anylabeling-win-gpu.spec`将本地的onnxruntime-gpu的相关动态库`*.dll`(Windows下) or `*.so`(Linux下) 添加进列表的`datas`参数中
    4. 安装的`onnxruntime-gpu`需要和`cuda`和`cudnn`适配，三者关系见<https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements>
    5. 若需要远古版本cudnn可以进入<https://developer.nvidia.com/rdp/cudnn-archive>
    6. 配置cuda和cudnn的方法可以在各技术论坛中找到

## 4月26日Q&A记录 <span id='index3'>
### Q：对现有的Wuhan_Metro数据集做什么标注？如何标注？
&emsp; 按照目前已有标签和PET方法的需求，标注头部点是更好的。不过如果提出一个新的架构，可能考虑矩形框等其它标记。可以在相关模型，如预训练的PET模型基础上自动标注，再手工调整。(我们目前打算先跑通PET测试性能，因此计划基于PET进行半自动的点标注)。

&emsp; 用于测试的数据，不需标注太多。可以在不同场景的视频中，在不同人群密度的条件下分别截取短片段进行标注。预计选取5段视频进行10个情况下的标记。
****
### Q：可以采用怎样的baseline?视频人群计数相关邻域有什么可借鉴方法和模型？
&emsp; Wuhan_Metro可以提供人群稠密与稀疏状况下的数据，题目要求以此为基础测试Point-Query_Quadtree方法的性能，并微调以适应不同任务。可以在PET方法为单帧图像计数的基础上，结合帧间融合和运动追踪等方法，实现时空一致性。也可以采取其它论文预测方法，有待讨论。

&emsp; 与此任务相近的Benchmark是一个基于头部检测的计数任务，详见<https://motchallenge.net/method/HT=2&chl=21>可以用HT21数据集进行模型训练，而地铁数据主要用于测试和演示。

&emsp; 可以从视频物体识别的论文方法中找一些视频跟踪算法的灵感，详见<https://github.com/codingonion/awesome-video-object-detection>(我们查看了部分demo，有一些方法时空一致性可以做得很好，考虑借鉴)。
****

## 5月27日更新
&emsp; 之前使用PET模型pytorch源码导出一张图片的点标注json文件，但出现了标签错位的情况。今天已经修改完成，并可以在原训练集ShanghaiTech上成功标注。

&emsp; 在WuhanMetro上也可运行并检测到一定数量行人，但效果欠佳。此后需微调模型改善其在任务数据集上的性能。鉴于我们目前还未确定Baseline，暂不确定是否用更改后的方法进行标注。



##  MOT(移动目标跟踪)领域文章阅读概述<span id='index4'>
&emsp; 由于没有找到足够有关视频人群计数领域的论文或开源方法，我们遵照建议阅读了一些MOT领域的文章，主要是CroHD(又称为Head Tracking21)数据集上的一些文章，其人群的跟踪可以算作MOT的一个子类，其跟踪目标是人，标签一般为人体矩形框或人头矩形框。在 [**重点文献总结**](#index1) 部分有对于这些文章的想法。

&emsp; 人头矩形框对于行人的数量检测的更好，因为在密集人群中，人体遮挡较多，显然容易漏检很多。我们会尽可能参考人头矩形框的跟踪方法。

&emsp; 人体矩形框的优势则是在检测到行人时，则可以对其动作也进行跟踪分析，例如Paddle Detection系列模型可以对行人的行为和特性进行分析。本任务并无此要求。所以只是在方法上借鉴，源代码需要修改。

&emsp; 查找目标跟踪领域的文章时，注意最终任务要求——检测计数。检测的准确性和帧间的稳定性应作为衡量该模型的关键因素，而类似IDEucl，用于检测跟踪轨迹的准确性，在本任务中重要性次之。同时，多数模型为了动作捕获和更好的跟踪，往往选取人体框标记，这也和我们的任务相悖。参考模型应该在跟踪和检测的准确性上达到平衡。

## 确立Baseline