# 进度记录和经验总结
## 本文档用于记录项目搭建的过程，遇到的问题及解决方案，以便后续查阅

## 目录
- [重点文献总结](#index1)
- [配置自动标注软件X-AnyLabeling](#index2)
- [4月26日Q&A记录](#index3)
- [MOT(移动目标跟踪)领域文章阅读概述](#index4)
- [确立Baseline](#index5)
- [掌握并运用PET方法](#index6)
- [数据标注工作](#index7)
- [Metro数据特性的探讨](#index8)
- [6月22日答疑总结](#index9)
- [FRVCC模型搭建](#index10)

## 重点文献总结 <span id='index1'>
1.`Sundararaman, Ramana, et al. "Tracking pedestrian heads in dense crowd." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.`

论文地址：<https://arxiv.org/pdf/2103.13516>

Github项目地址：<https://github.com/Sentient07/HeadHunter--T>

&emsp; 一篇比较综合的开源文章。提出了一个数据集`CroHD`，一个用于度量标准`IDEucl`，一个头部检测器`HeadHunter`以及以它为基础的头部跟踪方法`HeadHunter-T`。目前已有较多模型使用该数据集，并在结果上超过了本文，不过其中可以找到的开源方法并不多。

&emsp; 注：基于PET的点标注方法只可以用于检测，而跟踪显然需要更多信息。计数任务则介于两者之间，要求时空一致的检测，又不必完全准确地跟踪每个人。本文提供了一个新的思路，即，如果以头部框标注，则有可能结合头部特征进行跟踪，从而降级到人群计数，这和之前的每一帧头像检测，升级到时空一致计数是两个不同的方向。

2.`Cao, Jinkun et al. “Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking.” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 9686-9696.`

论文地址：<https://arxiv.org/pdf/2203.14360>

Github项目地址：<https://github.com/noahcao/OC_SORT>

&emsp; 文章提出的OC-SORT模型是对SORT（Simple online and realtime tracking）的一种改进，增强了其在遮挡和非线性运动(如舞蹈动作跟踪)任务上的表现。为了准确预测，加之大部分MOT数据集要求人体识别，所以其采用了全身的标注框。但是它也在HeadTracking数据集上做了一些工作。

&emsp; 该作者也提出，人群的跟踪基于头部会更加有效。据作者说，该方法在人体特定部位的识别上比之前的方法要更好，尽管使用的OC-SORT在其它MOT数据集上的参数。这一部分提及内容较少，但也许OC-SORT可以作为方法指引。

3.



## 配置自动标注软件X-AnyLabeling <span id='index2'>
&emsp; X-Anylabeling提供手动标注图片视频，及使用AI模型自动标注功能。下面是配置该软件的一些总结。
- 项目地址：<https://github.com/CVHub520/X-AnyLabeling>
- 按照项目README-get_started部分内容安装相关包
- 运行app.py以进入软件的GUI，即可在CPU模式下下载模型并进行标注。标注会在当前目录生成同名json文件
- 配置GPU运行步骤
    1. 将`anylabeling/app_info.py`中的`__preferred_device__` 参数改为GPU
    2. 安装相关包`pip install -r requirements-gpu-dev.txt`
    3. 打开`anylabeling-win-gpu.spec`将本地的onnxruntime-gpu的相关动态库`*.dll`(Windows下) or `*.so`(Linux下) 添加进列表的`datas`参数中
    4. 安装的`onnxruntime-gpu`需要和`cuda`和`cudnn`适配，三者关系见<https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements>
    5. 若需要远古版本cudnn可以进入<https://developer.nvidia.com/rdp/cudnn-archive>
    6. 配置cuda和cudnn的方法可以在各技术论坛中找到

## 4月26日Q&A记录 <span id='index3'>
### Q：对现有的Wuhan_Metro数据集做什么标注？如何标注？
&emsp; 按照目前已有标签和PET方法的需求，标注头部点是更好的。不过如果提出一个新的架构，可能考虑矩形框等其它标记。可以在相关模型，如预训练的PET模型基础上自动标注，再手工调整。(我们目前打算先跑通PET测试性能，因此计划基于PET进行半自动的点标注)。

&emsp; 用于测试的数据，不需标注太多。可以在不同场景的视频中，在不同人群密度的条件下分别截取短片段进行标注。预计选取5段视频进行10个情况下的标记。
****
### Q：可以采用怎样的baseline?视频人群计数相关邻域有什么可借鉴方法和模型？
&emsp; Wuhan_Metro可以提供人群稠密与稀疏状况下的数据，题目要求以此为基础测试Point-Query_Quadtree方法的性能，并微调以适应不同任务。可以在PET方法为单帧图像计数的基础上，结合帧间融合和运动追踪等方法，实现时空一致性。也可以采取其它论文预测方法，有待讨论。

&emsp; 与此任务相近的Benchmark是一个基于头部检测的计数任务，详见<https://motchallenge.net/method/HT=2&chl=21>可以用HT21数据集进行模型训练，而地铁数据主要用于测试和演示。

&emsp; 可以从视频物体识别的论文方法中找一些视频跟踪算法的灵感，详见<https://github.com/codingonion/awesome-video-object-detection>(我们查看了部分demo，有一些方法时空一致性可以做得很好，考虑借鉴)。
****

## 5月27日更新
&emsp; 之前使用PET模型pytorch源码导出一张图片的点标注json文件，但出现了标签错位的情况。今天已经修改完成，并可以在原训练集ShanghaiTech上成功标注。

&emsp; 在WuhanMetro上也可运行并检测到一定数量行人，但效果欠佳。此后需微调模型改善其在任务数据集上的性能。鉴于我们目前还未确定Baseline，暂不确定是否用更改后的方法进行标注。



##  MOT(移动目标跟踪)领域文章阅读概述<span id='index4'>
&emsp; 由于没有找到足够有关视频人群计数领域的论文或开源方法，我们遵照建议阅读了一些MOT领域的文章，主要是CroHD(又称为Head Tracking21)数据集上的一些文章，其人群的跟踪可以算作MOT的一个子类，其跟踪目标是人，标签一般为人体矩形框或人头矩形框。在 [**重点文献总结**](#index1) 部分有对于这些文章的想法。

&emsp; 人头矩形框对于行人的数量检测的更好，因为在密集人群中，人体遮挡较多，显然容易漏检很多。我们会尽可能参考人头矩形框的跟踪方法。

&emsp; 人体矩形框的优势则是在检测到行人时，则可以对其动作也进行跟踪分析，例如Paddle Detection系列模型可以对行人的行为和特性进行分析。本任务并无此要求。所以只是在方法上借鉴，源代码需要修改。

&emsp; 查找目标跟踪领域的文章时，注意最终任务要求——检测计数。检测的准确性和帧间的稳定性应作为衡量该模型的关键因素，而类似IDEucl，用于检测跟踪轨迹的准确性，在本任务中重要性次之。同时，多数模型为了动作捕获和更好的跟踪，往往选取人体框标记，这也和我们的任务相悖。参考模型应该在跟踪和检测的准确性上达到平衡。

&emsp; 跟踪任务的主要指标包括MOTA（跟踪准确性）、IDEucl（跟踪轨迹与真实轨迹的比例）等，这些指标与任务无关。而跟踪中的检测任务指标包括Precision、Recall、F1等准确度指标，这和传统计数中的MAE、MSE指标不一致。


## 确立Baseline <span id='index5'>
&emsp; 跟踪任务和检测任务属于比计数复杂的任务，如果基于跟踪和检测进行计数，则可能在其它无关方面分配无用权重，而在计数方面性能欠佳。我们讨论认为不应当将任务偏移到跟踪或检测上。PET模型和基于密度图的视频计数方案则暂时没有开源的参考方法，需要自己复现一些方法，或尝试别的自己构建。我们将以PET模型为基准，在Wuhan_Metro数据集进行标注和微调，先检验效果，然后尝试加入帧间一致性模块。

&emsp; 一个方向是测试PET的性能，再通过帧间约束，或用LSTM等记忆网络捕捉帧间关系用于计数的时空一致性；另一个是复现FRVCC论文中的整体框架——这是目前最符合我们需求的一种方法，但是没有开源，我们需要自行构建。另外，检测和跟踪方法也可尝试，但仅作为上述的借鉴。

## 掌握并运用PET方法 <span id='index6'>
&emsp; 我们通读了Point-Query-Quadtree方法论文，并通过开源代码搭建了环境。后续将使用PET在不同数据集上进行训练和测试。这是课题任务的一部分，也是我们将

## 数据标注工作 <span id='index7'>
&emsp;  由于地铁任务数据较少，我们在Wuhan_Metro上需要用于检验和训练的数据，故启动数据标注工作，并在此记录一些问题和体会。

&emsp;  我们标签的内容分为“Pedestrian”、“Inflow”和“Outflow”三类，点标注所有可见行人的头部中心位置。这样不仅标出所有行人，也标记了每一帧与相邻帧对比的进出状况。每两秒取一帧标注。这样做的一些问题：
+ 总的来说，进出的gt应当符合这样的约束：上一帧的Pedestrian+Inflow-Outflow和下一帧的Pedestrian+Outflow应保持一致，但标注时并没有严格遵照这一约束。
+ 在人群通行较快的区域，两秒时间内行人运动幅度较大，不好捕捉运动情况，标注的inflow和outflow可能不准确，会对应到其他人身上。
+ 在一些场景下，行人的头部会在画面内外来回摆动。通过观察其身体，可以确定其一直在画面中，但只标注头部时，可能出现“伪进出”情况。
+ 身份模糊。我们有时通过行人的衣着、面貌、体型等确认身份。在密集区域，或其它情况造成的上述信息无法辨认时，就只能标注Pedestrian，而不能确认是否进出。例如，连续几帧中有一队警卫员列队通过，此时无法看出警卫身份，也不能通过数量判断进出。
+ 图像本身存在不清晰、强光、遮挡等问题，会造成有些人的头部捕捉不到。对于远处模糊的小的头部，也难以给出准确位置标注。标注质量相应下降。存在镜面的地方则会产生难以辨识的人体的像。
+ 数据多样性较少，相似数据重复较多

&emsp; 以上部分问题也会影响模型的学习和检测，需要相应改进策略。

&emsp; 从上述进出的约束问题可想到，由于人员进出，视频中不同帧的计数ground-truth本身就可能变化较大，因此输出计数量的平稳性或MAE值并不代表计数的时空一致性。我们可以提出将一段视频中每张图片的实际误差加以统计，然后通过其变动的剧烈程度反映一致性。比如假设5张图片的output-gt为a1、a2、a3、a4、a5，那么这五个数两两都不应该发生剧烈变化，而作为整体，它们也应该在0附近波动。这样的指标反映模型预测与gt的接近程度，也就反映了时空一致性。

&emsp; 整理好的图片为1445张，来自5段视频，每段之间数据分布差异较大，每段之内均为10分钟的视频截取所得，因此数据分布很类似。我们将每段视频中提取1/4图片用于测试，其余3/4用于训练

&emsp; 6月22日更新：该训练方法存在问题。使用同样的数据进行训练/测试没有实际意义。即便拟合较好也无法应用。应当改为用4段视频作为训练，剩下一段或另取一段作为测试。

&emsp; 我们停止了数据标注工作。现在的工作方向改变为在质量较好的数据集，如JHU Crowd++上训练后，将自己标注的地铁数据集全部用作测试。



## Metro数据问题的探讨 <span id='index8'>
&emsp; 我们发现地铁数据集和先前用于训练的HT21数据集，或很多主流数据集在分布上具有较大差异，主要表现为：
- 深度差异较大。地铁摄像机位置较低，靠近镜头的人头部很大，而远离镜头的可以很小。因此，直接采用ShanghaiTechA数据集的预训练模型，其效果较差，难以检测到近处的头部。我们对PET模型的人头检测框大小进行调整，这改善了检测效果，可以定位更大的头部，但效果有限，过大的头部框对小的头部缺少捕获，从而MAE仍然会变高。
- 辨识度较差。部分视频区域难以判断是否有人通过，这会造成标注和检测的一些偏差。
- 以2s为间隔标注的数据可能缺乏帧间联系，所以不易用于训练或微调。最终测试可能需要直接输入视频(约25帧)，从中提取部分帧与之间标注数据进行检验和数据分析。

## 6月22日答疑总结 <span id='index9'>
&emsp; 除了上述数据集规划问题，我们还对接近尾声的工作确定了方向。

&emsp; 我们已经在PET模型上进行了较多的实验，可以在报告中对比这些实验结果，总结PET模型的检测准确性、泛化性、时空一致性等指标，探讨PET在地铁数据上的效果。

&emsp; 另一方面，目前在地铁数据集上的PET测试并没有达到满意、可以展示的效果，因此对PET模型要进行其它训练、微调加以改善，或尝试融合时空一致的模块以提高性能。

&emsp; 目前主流的各种视频计数方法依赖完善的视频数据集，并需要矩形框作为标注。对于直接使用矩形框信息进行计数的模型，我们由于数据集标注问题无法采用。但部分方法仅通过矩形框大小作为生成密度图的依据，在此情况下，点标注可以通过k-近邻方法替代矩形框来产生自适应密度图，这些方法可以借鉴采用。我们目前并不打算在标注的Metro数据集上训练，因此有较多模型可供选择。

&emsp; 鉴于时间并不充裕，我们打算率先搭建FRVCC模型，完全按照论文方法构建一个视频计数网络。它是以密度图为主要载体的，符合要求，且耦合性较小，我们可以直接将PET模型作为一个视觉计数模块嵌入该模型，将PET产生的头部标记输出转化成密度图。

## FRVCC模型搭建 <span id='index10'>