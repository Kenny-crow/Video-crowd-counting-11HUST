# 进度记录和经验总结
## 本文档用于记录项目搭建的过程，遇到的问题及解决方案，以便后续查阅

## 目录
- [文献查阅](#index1)
- [配置自动标注软件X-AnyLabeling](#index2)
- [4月26日Q&A记录](#index3)

## 文献查阅 <span id='index1'>
1. 


## 配置自动标注软件X-AnyLabeling <span id='index2'>
&emsp; X-Anylabeling提供手动标注图片视频，及使用AI模型自动标注功能。下面是配置该软件的一些总结。
- 项目地址：<https://github.com/CVHub520/X-AnyLabeling>
- 按照项目README-get_started部分内容安装相关包
- 运行app.py以进入软件的GUI，即可在CPU模式下下载模型并进行标注。标注会在当前目录生成同名json文件
- 配置GPU运行步骤
    1. 将`anylabeling/app_info.py`中的`__preferred_device__` 参数改为GPU
    2. 安装相关包`pip install -r requirements-gpu-dev.txt`
    3. 打开`anylabeling-win-gpu.spec`将本地的onnxruntime-gpu的相关动态库`*.dll`(Windows下) or `*.so`(Linux下) 添加进列表的`datas`参数中
    4. 安装的`onnxruntime-gpu`需要和`cuda`和`cudnn`适配，三者关系见<https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements>
    5. 若需要远古版本cudnn可以进入<https://developer.nvidia.com/rdp/cudnn-archive>
    6. 配置cuda和cudnn的方法可以在各技术论坛中找到

## 4月26日Q&A记录 <span id='index3'>
### Q：对现有的Wuhan_Metro数据集做什么标注？如何标注？
&emsp; 按照目前已有标签和PET方法的需求，标注头部点是更好的。不过如果提出一个新的架构，可能考虑矩形框等其它标记。可以在相关模型，如预训练的PET模型基础上自动标注，再手工调整。(我们目前打算先跑通PET测试性能，因此计划基于PET进行半自动的点标注)。

&emsp; 用于测试的数据，不需标注太多。可以在不同场景的视频中，在不同人群密度的条件下分别截取短片段进行标注。预计选取5段视频进行10个情况下的标记。
****
### Q：可以采用怎样的baseline?视频人群计数相关邻域有什么可借鉴方法和模型？
&emsp; Wuhan_Metro可以提供人群稠密与稀疏状况下的数据，题目要求以此为基础测试Point-Query_Quadtree方法的性能，并微调以适应不同任务。可以在PET方法为单帧图像计数的基础上，结合帧间融合和运动追踪等方法，实现时空一致性。也可以采取其它论文预测方法，有待讨论。

&emsp; 与此任务相近的Benchmark是一个基于头部检测的计数任务，详见<https://motchallenge.net/method/HT=2&chl=21>可以用HT21数据集进行模型训练，而地铁数据主要用于测试和演示。

&emsp; 可以从视频物体识别的论文方法中找一些视频跟踪算法的灵感，详见<https://github.com/codingonion/awesome-video-object-detection>(我们查看了部分demo，有一些方法时空一致性可以做得很好，考虑借鉴)。
****